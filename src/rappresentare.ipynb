{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cosa stiamo osservando?*\n",
    "\n",
    "*Quale fenomeno stiamo cercando di descrivere?*\n",
    "\n",
    "*Cosa sto sperimentando?*\n",
    "\n",
    "... e poi ...\n",
    "\n",
    "*Quali sono le mie ipotesi?*\n",
    "\n",
    "*Quali sono le ipotesi della comunità scientifica?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo l'ambiente giusto, import cose (skip)\n",
    "\n",
    "Qui dentro ci sono cose che non sono necessarie allo scopo della lezione. \n",
    "Chi ha interesse può certamente dare un'occhiata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%pip install torchaudio librosa boto3 pillow scipy matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "torch.set_num_threads(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Preparation of data and helper functions.\n",
    "#-------------------------------------------------------------------------------\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "import librosa\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import requests\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "[width, height] = matplotlib.rcParams['figure.figsize']\n",
    "if width < 10:\n",
    "  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n",
    "\n",
    "_SAMPLE_DIR = \"../res/_sample_data\"\n",
    "SAMPLE_WAV_URL = \"http://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\n",
    "SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n",
    "\n",
    "SAMPLE_WAV_SPEECH_URL = \"http://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n",
    "\n",
    "SAMPLE_RIR_URL = \"http://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n",
    "SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n",
    "\n",
    "SAMPLE_NOISE_URL = \"http://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n",
    "SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n",
    "\n",
    "SAMPLE_MP3_URL = \"http://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.mp3\"\n",
    "SAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n",
    "\n",
    "SAMPLE_GSM_URL = \"http://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.gsm\"\n",
    "SAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n",
    "\n",
    "SAMPLE_TAR_URL = \"http://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit.tar.gz\"\n",
    "SAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\n",
    "SAMPLE_TAR_ITEM = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "S3_BUCKET = \"pytorch-tutorial-assets\"\n",
    "S3_KEY = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n",
    "os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(_SAMPLE_DIR, exist_ok=True)\n",
    "\n",
    "def _fetch_data():\n",
    "  uri = [\n",
    "    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n",
    "    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n",
    "    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n",
    "    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
    "    (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n",
    "    (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n",
    "    (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n",
    "  ]\n",
    "  for url, path in uri:\n",
    "    with open(path, 'wb') as file_:\n",
    "      file_.write(requests.get(url).content)\n",
    "\n",
    "_fetch_data()\n",
    "\n",
    "def _download_yesno():\n",
    "  if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n",
    "    return\n",
    "  torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n",
    "\n",
    "YESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\n",
    "YESNO_DOWNLOAD_PROCESS.start()\n",
    "\n",
    "def _get_sample(path, resample=None):\n",
    "  effects = [\n",
    "    [\"remix\", \"1\"]\n",
    "  ]\n",
    "  if resample:\n",
    "    effects.extend([\n",
    "      [\"lowpass\", f\"{resample // 2}\"],\n",
    "      [\"rate\", f'{resample}'],\n",
    "    ])\n",
    "  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "\n",
    "def get_speech_sample(*, resample=None):\n",
    "  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n",
    "\n",
    "def get_sample(*, resample=None):\n",
    "  return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
    "\n",
    "def get_rir_sample(*, resample=None, processed=False):\n",
    "  rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n",
    "  if not processed:\n",
    "    return rir_raw, sample_rate\n",
    "  rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
    "  rir = rir / torch.norm(rir, p=2)\n",
    "  rir = torch.flip(rir, [1])\n",
    "  return rir, sample_rate\n",
    "\n",
    "def get_noise_sample(*, resample=None):\n",
    "  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n",
    "\n",
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "  if src:\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", src)\n",
    "    print(\"-\" * 10)\n",
    "  if sample_rate:\n",
    "    print(\"Sample Rate:\", sample_rate)\n",
    "  print(\"Shape:\", tuple(waveform.shape))\n",
    "  print(\"Dtype:\", waveform.dtype)\n",
    "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "  print()\n",
    "  print(waveform)\n",
    "  print()\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "    axes[c].grid(True)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "    if ylim:\n",
    "      axes[c].set_ylim(ylim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def play_audio(waveform, sample_rate):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  if num_channels == 1:\n",
    "    display(Audio(waveform[0], rate=sample_rate))\n",
    "  elif num_channels == 2:\n",
    "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "  else:\n",
    "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
    "\n",
    "def inspect_file(path):\n",
    "  print(\"-\" * 10)\n",
    "  print(\"Source:\", path)\n",
    "  print(\"-\" * 10)\n",
    "  print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
    "  print(f\" - {torchaudio.info(path)}\")\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Spectrogram (db)')\n",
    "  axs.set_ylabel(ylabel)\n",
    "  axs.set_xlabel('frame')\n",
    "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "  if xmax:\n",
    "    axs.set_xlim((0, xmax))\n",
    "  fig.colorbar(im, ax=axs)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_mel_fbank(fbank, title=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Filter bank')\n",
    "  axs.imshow(fbank, aspect='auto')\n",
    "  axs.set_ylabel('frequency bin')\n",
    "  axs.set_xlabel('mel bin')\n",
    "  plt.show(block=False)\n",
    "\n",
    "def get_spectrogram(\n",
    "    n_fft = 400,\n",
    "    win_len = None,\n",
    "    hop_len = None,\n",
    "    power = 2.0,\n",
    "):\n",
    "  waveform, _ = get_speech_sample()\n",
    "  spectrogram = T.Spectrogram(\n",
    "      n_fft=n_fft,\n",
    "      win_length=win_len,\n",
    "      hop_length=hop_len,\n",
    "      center=True,\n",
    "      pad_mode=\"reflect\",\n",
    "      power=power,\n",
    "  )\n",
    "  return spectrogram(waveform)\n",
    "\n",
    "def plot_pitch(waveform, sample_rate, pitch):\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.set_title(\"Pitch Feature\")\n",
    "  axis.grid(True)\n",
    "\n",
    "  end_time = waveform.shape[1] / sample_rate\n",
    "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
    "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
    "\n",
    "  axis2 = axis.twinx()\n",
    "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "  ln2 = axis2.plot(\n",
    "      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
    "\n",
    "  axis2.legend(loc=0)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.set_title(\"Kaldi Pitch Feature\")\n",
    "  axis.grid(True)\n",
    "\n",
    "  end_time = waveform.shape[1] / sample_rate\n",
    "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
    "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
    "\n",
    "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
    "  axis.set_ylim((-1.3, 1.3))\n",
    "\n",
    "  axis2 = axis.twinx()\n",
    "  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n",
    "  ln2 = axis2.plot(\n",
    "      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n",
    "\n",
    "  lns = ln1 + ln2\n",
    "  labels = [l.get_label() for l in lns]\n",
    "  axis.legend(lns, labels, loc=0)\n",
    "  plt.show(block=False)\n",
    "\n",
    "DEFAULT_OFFSET = 201\n",
    "SWEEP_MAX_SAMPLE_RATE = 48000\n",
    "DEFAULT_LOWPASS_FILTER_WIDTH = 6\n",
    "DEFAULT_ROLLOFF = 0.99\n",
    "DEFAULT_RESAMPLING_METHOD = 'sinc_interpolation'\n",
    "\n",
    "def _get_log_freq(sample_rate, max_sweep_rate, offset):\n",
    "  \"\"\"Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]\n",
    "\n",
    "  offset is used to avoid negative infinity `log(offset + x)`.\n",
    "\n",
    "  \"\"\"\n",
    "  half = sample_rate // 2\n",
    "  start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)\n",
    "  return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset\n",
    "\n",
    "def _get_inverse_log_freq(freq, sample_rate, offset):\n",
    "  \"\"\"Find the time where the given frequency is given by _get_log_freq\"\"\"\n",
    "  half = sample_rate // 2\n",
    "  return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))\n",
    "\n",
    "def _get_freq_ticks(sample_rate, offset, f_max):\n",
    "  # Given the original sample rate used for generating the sweep,\n",
    "  # find the x-axis value where the log-scale major frequency values fall in\n",
    "  time, freq = [], []\n",
    "  for exp in range(2, 5):\n",
    "    for v in range(1, 10):\n",
    "      f = v * 10 ** exp\n",
    "      if f < sample_rate // 2:\n",
    "        t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate\n",
    "        time.append(t)\n",
    "        freq.append(f)\n",
    "  t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate\n",
    "  time.append(t_max)\n",
    "  freq.append(f_max)\n",
    "  return time, freq\n",
    "\n",
    "def plot_sweep(waveform, sample_rate, title, max_sweep_rate=SWEEP_MAX_SAMPLE_RATE, offset=DEFAULT_OFFSET):\n",
    "  x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]\n",
    "  y_ticks = [1000, 5000, 10000, 20000, sample_rate//2]\n",
    "\n",
    "  time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)\n",
    "  freq_x = [f if f in x_ticks and f <= max_sweep_rate // 2 else None for f in freq]\n",
    "  freq_y = [f for f in freq if f >= 1000 and f in y_ticks and f <= sample_rate // 2]\n",
    "\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.specgram(waveform[0].numpy(), Fs=sample_rate)\n",
    "  plt.xticks(time, freq_x)\n",
    "  plt.yticks(freq_y, freq_y)\n",
    "  axis.set_xlabel('Original Signal Frequency (Hz, log scale)')\n",
    "  axis.set_ylabel('Waveform Frequency (Hz)')\n",
    "  axis.xaxis.grid(True, alpha=0.67)\n",
    "  axis.yaxis.grid(True, alpha=0.67)\n",
    "  figure.suptitle(f'{title} (sample rate: {sample_rate} Hz)')\n",
    "  plt.show(block=True)\n",
    "\n",
    "def get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):\n",
    "    max_sweep_rate = sample_rate\n",
    "    freq = _get_log_freq(sample_rate, max_sweep_rate, offset)\n",
    "    delta = 2 * math.pi * freq / sample_rate\n",
    "    cummulative = torch.cumsum(delta, dim=0)\n",
    "    signal = torch.sin(cummulative).unsqueeze(dim=0)\n",
    "    return signal\n",
    "\n",
    "def benchmark_resample(\n",
    "    method,\n",
    "    waveform,\n",
    "    sample_rate,\n",
    "    resample_rate,\n",
    "    lowpass_filter_width=DEFAULT_LOWPASS_FILTER_WIDTH,\n",
    "    rolloff=DEFAULT_ROLLOFF,\n",
    "    resampling_method=DEFAULT_RESAMPLING_METHOD,\n",
    "    beta=None,\n",
    "    librosa_type=None,\n",
    "    iters=5\n",
    "):\n",
    "  if method == \"functional\":\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
    "                 rolloff=rolloff, resampling_method=resampling_method)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters\n",
    "  elif method == \"transforms\":\n",
    "    resampler = T.Resample(sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
    "                           rolloff=rolloff, resampling_method=resampling_method, dtype=waveform.dtype)\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      resampler(waveform)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters\n",
    "  elif method == \"librosa\":\n",
    "    waveform_np = waveform.squeeze().numpy()\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      librosa.resample(waveform_np, sample_rate, resample_rate, res_type=librosa_type)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La rappresentazione non dipende dall'oggetto da rappresentare (pensiamo a Magritte), è un simbolo (syn-bálleyn, stessa radice di dya-bálleyn: diaboloco) che di per sè lascia il fenomeno che stiamo descrivendo indifferenziato da tutto il resto, in particolare dal suo opposto o contrario.\n",
    "\n",
    "Il simbolo può essere l'immagine (una foto, un disegno), il testo (una parola scritta), il suono (una voce che pronuncia la parola oppure una onomatopeica).\n",
    "**Questi simboli richiamano alla nostra memoria l'oggetto che vogliamo rappresentare**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecniche per la rappresentazione di immagini\n",
    "\n",
    "Come si rappresenta un'immagine per l'apprendimento automatico? Per rispondere dobbiamo comprendere come rappresentiamo le immagini per la macchina.\n",
    "Questo non sono altro che **matrici** i cui elementi (i *pixel*) corrispondono a valori di intensità di uno o più canali colore.\n",
    "Avrete sentito parlare di canali *RGB* (*Red*, *Green* and *Blue* channels) per le immagini a colori oppure .\n",
    "Ogni matrice rappresenta un canale colore e ogni elemento della matrice rappresenta un pixel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the image file for reading\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/thezingaro/mast-summer-school-ai/main/res/me.jpg', 'me.jpg')\n",
    "img = Image.open('me.jpg')\n",
    "\n",
    "# get the original size of the image\n",
    "width, height = img.size\n",
    "print(width, height)\n",
    "\n",
    "# get the nearest power of 2 for the width and height\n",
    "width = height = int(2 ** (width.bit_length() - 1))\n",
    "print(width, height)\n",
    "\n",
    "# crop the image with the new width and height\n",
    "img = img.crop((0, 0, width, height))\n",
    "\n",
    "# resize the image to an half of the original width and height\n",
    "img = img.resize((width // 2, height // 2))\n",
    "\n",
    "# display the image in the notebook\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the image to a numpy array\n",
    "img_array = np.array(img)\n",
    "print(img_array.shape)\n",
    "\n",
    "# convert the central pixels of the numpy array to a PIL image\n",
    "img_array = img_array[20:276, 256:512]\n",
    "\n",
    "# print the shape of the array\n",
    "print(img_array.shape)\n",
    "\n",
    "# convert the numpy array to a PIL image\n",
    "img_pil = Image.fromarray(img_array)\n",
    "\n",
    "# display the image in the notebook\n",
    "display(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the image in black and white\n",
    "img_pil_bw = img_pil.convert('1') # try out 'L' for grayscale, '1' for black and white, 'RGB' for color, 'LA' for grayscale with alpha, 'RGBA' for color with alpha\n",
    "\n",
    "# convert the image to a numpy array\n",
    "img_array_bw = np.array(img_pil_bw)\n",
    "print(img_array_bw.shape)\n",
    "\n",
    "# display the image in the notebook\n",
    "display(img_pil_bw)\n",
    "\n",
    "# print the numpy array of the image\n",
    "print(img_array_bw)\n",
    "\n",
    "# save the image to a file named \"small.png\"\n",
    "img_pil_bw.save('small.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the sobel filtered image\n",
    "img_array_sobel = nd.sobel(img_array)\n",
    "img_sobel = Image.fromarray(img_array_sobel)\n",
    "display(img_sobel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the laplacian filtered image\n",
    "img_array_laplacian = nd.laplace(img_array)\n",
    "img_laplacian = Image.fromarray(img_array_laplacian)\n",
    "display(img_laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecniche per la rappresentazione di parole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segui [questo link](https://beta.openai.com/playground/p/default-qa?lang=curl&model=text-davinci-002) per interrogare un modello di AI (**DaVinci** di *GPT-3*) addestrata allo scopo di *completare il testo* scritto da parte dell'utente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecniche per la rappresentazione di suoni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito, (e)seguiamo alcune parti selezionate del *audio preprocessing tutorial* [disponibile al seguente link](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#audio-manipulation-with-torchaudio), degli sviluppatori di `PyTorch`, per capire come **rappresentare un segnale audio** per la macchina.\n",
    "\n",
    "Per i più curiosi: il dataset che useremo come esempio è il *Voices Obscured in Complex Environmental Settings* (VOiCES), [maggiori info a questo link](https://iqtlabs.github.io/voices/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Come (noi) rappresenta un suono per la macchina?\n",
    "\n",
    "Proviamo ad estrarre delle informazioni (metadati) sul file che rappresenta il suono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = torchaudio.info(SAMPLE_WAV_PATH)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where (from the tutorial)\n",
    "\n",
    "-  ``sample_rate`` is the sampling rate of the audio\n",
    "-  ``num_channels`` is the number of channels\n",
    "-  ``num_frames`` is the number of frames per channel\n",
    "-  ``bits_per_sample`` is bit depth\n",
    "-  ``encoding`` is the sample coding format\n",
    "\n",
    "The values ``encoding`` can take are one of the following\n",
    "\n",
    "-  ``\"PCM_S\"``: Signed integer linear PCM\n",
    "-  ``\"PCM_U\"``: Unsigned integer linear PCM\n",
    "-  ``\"PCM_F\"``: Floating point linear PCM\n",
    "-  ``\"FLAC\"``: Flac, `Free Lossless Audio\n",
    "   Codec <https://xiph.org/flac/>`__\n",
    "-  ``\"ULAW\"``: Mu-law,\n",
    "   [`wikipedia <https://en.wikipedia.org/wiki/%CE%9C-law_algorithm>`__]\n",
    "-  ``\"ALAW\"``: A-law\n",
    "   [`wikipedia <https://en.wikipedia.org/wiki/A-law_algorithm>`__]\n",
    "-  ``\"MP3\"`` : MP3, MPEG-1 Audio Layer III\n",
    "-  ``\"VORBIS\"``: OGG Vorbis [`xiph.org <https://xiph.org/vorbis/>`__]\n",
    "-  ``\"AMR_NB\"``: Adaptive Multi-Rate\n",
    "   [`wikipedia <https://en.wikipedia.org/wiki/Adaptive_Multi-Rate_audio_codec>`__]\n",
    "-  ``\"AMR_WB\"``: Adaptive Multi-Rate Wideband\n",
    "   [`wikipedia <https://en.wikipedia.org/wiki/Adaptive_Multi-Rate_Wideband>`__]\n",
    "-  ``\"OPUS\"``: Opus [`opus-codec.org <https://opus-codec.org/>`__]\n",
    "-  ``\"GSM\"``: GSM-FR\n",
    "   [`wikipedia <https://en.wikipedia.org/wiki/Full_Rate>`__]\n",
    "-  ``\"UNKNOWN\"`` None of above\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ascoltare un suono\n",
    "\n",
    "Selezioniamo un suono di esempio (una frase pronunciata in inglese) e [scarichiamo una copia locale](../res/_sample_data/speech.wav) oppure [visitiamo nel nostro browser preferito il link](https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav) che otteniamo dai metadati (e poi lo scarichiamo 😓)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Source:\", SAMPLE_WAV_SPEECH_PATH)\n",
    "metadata = torchaudio.info(SAMPLE_WAV_SPEECH_PATH)\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizzare un suono\n",
    "\n",
    "Per ottenere una rappresentazione visiva della frequenza di un segnale audio è possibile generare un'immagine a partire dal vettore di numeri che abbiamo usato per rappresentare il suono.\n",
    "\n",
    "Tra le tante tecniche che si possono implementare, visualizziamo la semplice **forma d'onda** e lo **spettrogramma**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(SAMPLE_WAV_SPEECH_PATH)\n",
    "\n",
    "print_stats(waveform, sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waveform(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_specgram(waveform, sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
